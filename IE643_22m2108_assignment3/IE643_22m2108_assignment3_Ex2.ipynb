{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a3a4bc",
   "metadata": {},
   "source": [
    "# NOTE TO THE EVALUATOR, PLEASE READ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db750c",
   "metadata": {},
   "source": [
    "The notebook takes data from a local directory namely <code>folder_path = './Assignment3_dataset/' </code>. It was suggested in the course discussion forum to upload data along with the notebook. As the dataset provided was of 82 MB , it was impossible to upload it on moodle(Limit exceded).<br><br>\n",
    "This code will run properly in any directory where the extracted folder <code>Assignment3_dataset</code> is present. Futhermore All the plots and images are already presented by submittiong the executed notebook.<br><br>\n",
    "All the imports that are used in this notebook are colledted together in the Import subsection below. All these imported smoothly on Google Collab and no error or requirement is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe81244",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90597d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68168d7",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e977341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, batch=True, save=False, title='one'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots batch of images as well as normal single image \n",
    "    Input is X : torch tensor : shape batch x 3 x h x w\n",
    "    \"\"\"\n",
    "    \n",
    "    if batch:\n",
    "        # Assuming you have a batch of images with shape Bx3x64x64\n",
    "        num_cols = 8  # Specify the number of rows and columns in the grid\n",
    "        num_rows = len(X)//num_cols if len(X)//num_cols else 1 \n",
    "\n",
    "        # Create a figure and a grid of subplots to display the images\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols, num_rows))\n",
    "\n",
    "        # Iterate through the batch and plot each image in the grid\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                idx = i * num_cols + j\n",
    "                image = X[idx].permute(1, 2, 0).numpy()  # Convert to HxWxC format\n",
    "                axs[i, j].imshow((image*255).astype(\"uint8\"))\n",
    "                axs[i, j].axis('off')\n",
    "\n",
    "        # Adjust spacing and layout\n",
    "        # Reduce spacing between subplots\n",
    "        plt.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "        \n",
    "        # Save the subplots as an image\n",
    "        if save:\n",
    "            plot_name = title + '.png' \n",
    "            plt.savefig(plot_name)\n",
    "            \n",
    "    else:\n",
    "        # Note here X should be an image 3xhxw\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(np.transpose(X, (1, 2, 0)))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image1, image2, scale=1):\n",
    "    \n",
    "    # images are torch tensors\n",
    "    image1 = np.transpose(image1.numpy(),(1,2,0))\n",
    "    image2 = np.transpose(image2.numpy(),(1,2,0))\n",
    "        \n",
    "    # Create a new figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(scale*8, scale*4))  # Create a 1x2 grid of subplots\n",
    "\n",
    "    # Plot image1 on the first subplot\n",
    "    axes[0].imshow(image1)\n",
    "    axes[0].axis('off')  # Turn off axis labels\n",
    "    axes[0].set_title(f'Initial Image')\n",
    "\n",
    "    # Plot image2 on the second subplot\n",
    "    axes[1].imshow(image2)\n",
    "    axes[1].axis('off')  # Turn off axis labels\n",
    "    axes[1].set_title(f'Transformed Image')\n",
    "\n",
    "    # Adjust spacing between the subplots\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "    # Show the combined figure with both images side by side\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac899ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, plot=False):\n",
    "    \n",
    "    if plot:\n",
    "        print(\"########## ORIGINAL IMAGES ##########\")\n",
    "        plot_images(X)\n",
    "\n",
    "    # Gamma balance the image\n",
    "    gamma = 1.8\n",
    "    X_gamma =  torch.clamp(X * gamma, 0, 1)\n",
    "    if plot:\n",
    "        print(\"########## GAMMA IMAGES ##########\")\n",
    "        plot_images(X_gamma)\n",
    "\n",
    "    # Sharpen the images\n",
    "    X_sharper = torchvision.transforms.functional.adjust_sharpness(X_gamma, sharpness_factor=3.5)\n",
    "    if plot:\n",
    "        print(\"########## SHARPER IMAGES ##########\")\n",
    "        plot_images(X_sharper)\n",
    "\n",
    "    # Rotate the images +5\n",
    "    X_rotated_plus = torchvision.transforms.functional.rotate(X_sharper,angle=5, interpolation= InterpolationMode.BILINEAR)\n",
    "    if plot:\n",
    "        print(\"########## +5 ROTATED IMAGES ##########\")\n",
    "        plot_images(X_rotated_plus)\n",
    "\n",
    "    # Rotate the images -5\n",
    "    X_rotated_minus = torchvision.transforms.functional.rotate(X_sharper,angle=-5, interpolation= InterpolationMode.BILINEAR)\n",
    "    if plot:\n",
    "        print(\"########## -5 ROTATED IMAGES ##########\")\n",
    "        plot_images(X_rotated_minus) \n",
    "\n",
    "    # Concatenate images\n",
    "    X_out = torch.cat((X_sharper, X_rotated_plus, X_rotated_minus), dim=0)\n",
    "\n",
    "    # Flip the images\n",
    "    X_flipped = torchvision.transforms.functional.hflip(X_sharper)\n",
    "    if plot:\n",
    "        print(\"########## FLIPPED IMAGES ##########\")\n",
    "        plot_images(X_flipped) \n",
    "\n",
    "    # Rotate the flipped images +5\n",
    "    X_flipped_rotated_plus = torchvision.transforms.functional.rotate(X_flipped ,angle=5, interpolation= InterpolationMode.BILINEAR)\n",
    "    if plot:\n",
    "        print(\"########## FLIPPED +5 ROTATED IMAGES ##########\")\n",
    "        plot_images(X_flipped_rotated_plus)\n",
    "\n",
    "    # Rotate the flipped images -5\n",
    "    X_flipped_rotated_minus = torchvision.transforms.functional.rotate(X_flipped ,angle=-5, interpolation= InterpolationMode.BILINEAR)\n",
    "    if plot:\n",
    "        print(\"########## FLIPPED -5 ROTATED IMAGES ##########\")\n",
    "        plot_images(X_flipped_rotated_minus)\n",
    "\n",
    "    # Concatenate images\n",
    "    X_out = torch.cat((X_out , X_flipped, X_flipped_rotated_plus, X_flipped_rotated_minus), dim=0)\n",
    "    \n",
    "    return X_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f68d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_val=42):\n",
    "    # Set the seed for the random number generator in PyTorch\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    # Set the seed for the random number generator in the Python standard library (random module)\n",
    "    random.seed(seed_val)\n",
    "\n",
    "    # Set the seed for the random number generator in NumPy\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # Set the seed for the CUDA (GPU) operations if you are using GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Additional configurations for deterministic behavior (not always necessary)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c14d5",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "## For this question, choose a different object category K, different from C chosen in the previous question. Construct a data set D1 containing 90% images from the category K of the data. Construct a validation set V1 using the remaining 10% images. This question is about constructing a VAE to generate images similar to those in category K. Please note that you need to choose only a single object category for this exercise. Answers generating all three categories of objects will not be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc127780",
   "metadata": {},
   "source": [
    "### Read Folder Path and image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_class = 'boat'     # Choose Category 'boat' 'bus' or 'car' (here K)\n",
    "desired_width = 64\n",
    "desired_height = 64\n",
    "folder_path = './Assignment3_dataset/' \n",
    "\n",
    "class_names = os.listdir(folder_path)   # A list of images in folder path\n",
    "print(f'Total classes in folder : {len(class_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124d374",
   "metadata": {},
   "source": [
    "### Read All Images in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c08a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = datasets.ImageFolder(root=folder_path,\n",
    "                               transform=transforms.Compose([\n",
    "                               transforms.Resize((desired_height,desired_width)),\n",
    "                               transforms.ToTensor(),\n",
    "                           ]))\n",
    "\n",
    "\n",
    "# Filter data according to desired class\n",
    "print(f'Class to id dictionary : {dataset.class_to_idx}')\n",
    "chosen_class_id = dataset.class_to_idx[chosen_class]\n",
    "print(f'Index of the desired class : {chosen_class_id} ')\n",
    "\n",
    "class_indices = []\n",
    "for i in range(len(dataset)):\n",
    "    if dataset.imgs[i][1]==chosen_class_id:\n",
    "        class_indices.append(i)\n",
    "subset = Subset(dataset, class_indices)\n",
    "\n",
    "all_data = []\n",
    "for idx in range(len(subset)):\n",
    "    data, target = subset[idx]\n",
    "    all_data.append((data, target))\n",
    "\n",
    "d_loader = DataLoader(subset, batch_size=len(subset))\n",
    "for batch in d_loader:\n",
    "    X_all, _ = batch  # data and target are torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#################### SOME IMAGES FOR THE CATEGORY ####################\")\n",
    "plot_images(X_all[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2195da",
   "metadata": {},
   "source": [
    "### 90-10 split the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8064fa",
   "metadata": {},
   "source": [
    "## SOL : DATASET D1 and V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05008a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D1, V1 = train_test_split(X_all, test_size=0.1, random_state=42)\n",
    "print(f\"Images in the train set D1 : {D1.shape[0]}\")\n",
    "print(f\"Images in the test set V1 : {V1.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6ff98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Augment DATA for Training and Testing \n",
    "# Toggle 'plot=True' for viauslisation of augnemtations\n",
    "X_train = augment_data(D1, plot=False)\n",
    "X_train = X_train[:576] \n",
    "X_val = augment_data(V1, plot=False)\n",
    "X_val_norm = 2 * X_val - 1\n",
    "X_val_norm  = X_val_norm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc49b23",
   "metadata": {},
   "source": [
    "______________\n",
    "# (a)\n",
    "## Construct a suitable VAE architecture using convolutions and related operations (e.g. upsampling, pooling, batch normalization etc). You can use the demo code posted in moodle to construct your VAE. You are free to use other CNN architectures for your VAE based on the requirements of the data set. If you choose a different architecure, you must clearly justify your choice for the same.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b19073",
   "metadata": {},
   "source": [
    "## SOL (a) : Constructed VAE below : Please note that this is the same architecture as presented in the implementation provided. Only a new function is added to generate images. Also the code is written in a little modular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa656ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims = None,\n",
    "                 kld_w = 0.00025):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.kld_w = kld_w \n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            #hidden_dims = [32, 64, 128, 256, 512]\n",
    "            hidden_dims = [32, 64, 128]\n",
    "            \n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        # 4 comes from 2x2\n",
    "        # self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        # self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*64, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*64, latent_dim)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        #self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 64)\n",
    "        \n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(x)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        # 2,2 comes from 2x2\n",
    "        #result = result.view(-1, 512, 2, 2)\n",
    "        result = result.view(-1, 128, 8, 8)\n",
    "        \n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), x, mu, log_var]\n",
    "\n",
    "    def loss_function(self, args):\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        \n",
    "        kld_weight = self.kld_w # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self, num_samples, current_device):\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x):\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc7eb4",
   "metadata": {},
   "source": [
    "______________\n",
    "# (b)\n",
    "## Train the VAE on the data D1 .\n",
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf940eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000 \n",
    "patience = 120  # Number of epochs to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e321422",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "plot_idx=0\n",
    "for LR in [.001]:\n",
    "    for KLD_WEIGHT in [5e-5]:\n",
    "        \n",
    "        plot_idx +=1\n",
    "        title_x = str(plot_idx) + f\"_LR={LR}__KLD_WEIGHT={KLD_WEIGHT}__real\"\n",
    "        title_recon = str(plot_idx) + f\"_LR={LR}__KLD_WEIGHT={KLD_WEIGHT}__recon\"\n",
    "        title_gen = str(plot_idx) + f\"_LR={LR}__KLD_WEIGHT={KLD_WEIGHT}__sampled\"\n",
    "        \n",
    "        \n",
    "        seed_all(seed_val=42)\n",
    "        model = VAE(in_channels=3, latent_dim=256, kld_w=KLD_WEIGHT).to(device)\n",
    "        \n",
    "        optimizer_vae = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        # Define the learning rate scheduler\n",
    "        step_size = 10  # Adjust this according to your training schedule\n",
    "        gamma = 0.95  # Factor by which the learning rate is reduced\n",
    "        scheduler = StepLR(optimizer_vae, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        \n",
    "        train_loader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        best_test_loss = float('inf')\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            train_loss = 0\n",
    "            recon_loss = 0\n",
    "            kl_loss = 0\n",
    "            recon_batch = None\n",
    "\n",
    "            model.train()\n",
    "            for batch_idx, X_batch in enumerate(train_loader):\n",
    "\n",
    "\n",
    "                X_norm = 2 * X_batch - 1\n",
    "                X_norm = X_norm.to(device)\n",
    "\n",
    "                optimizer_vae.zero_grad()\n",
    "                decoded, x, mu, log_var = model(X_norm)\n",
    "                arr = [decoded, x, mu, log_var]\n",
    "\n",
    "                loss_dict = model.loss_function(arr)\n",
    "\n",
    "                reconstruction_loss = loss_dict['Reconstruction_Loss']\n",
    "                recon_loss += reconstruction_loss\n",
    "\n",
    "                kl_divergence_loss = loss_dict['KLD']\n",
    "                kl_loss += kl_divergence_loss\n",
    "\n",
    "                loss = loss_dict['loss']\n",
    "                loss.backward()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                optimizer_vae.step()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # After Every Epoch\n",
    "            scheduler.step()\n",
    "\n",
    "            ##### USING VALIDATION SET #####\n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            \n",
    "            if epoch%25==0:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    \n",
    "\n",
    "                    print(f'EPOCH : {epoch+1}')\n",
    "                    print(f'RECON LOSS : {reconstruction_loss} --- KL LOSS : {kl_divergence_loss} --- LOSS : {loss.item()}')\n",
    "                    print(\"######################## STILL TRAINING ########################\")\n",
    "\n",
    "                    print(\"####################### INPUT IMAGES #######################\")\n",
    "                    plot_images((((X_norm.detach().cpu())+1.0)/2.0)[:16])\n",
    "\n",
    "                    print(\"####################### RECON IMAGES #######################\")\n",
    "                    recons = model.generate(X_norm).detach().cpu()\n",
    "                    recons = (recons+1.0)/2.0\n",
    "                    plot_images(recons[:16])\n",
    "\n",
    "                    print(\"####################### GEN IMAGES #######################\")\n",
    "                    sampled = model.sample(num_samples=64, current_device=device)\n",
    "                    plot_images((((sampled.detach().cpu())+1.0)/2.0)[:16])\n",
    "                    \n",
    "                    print(\"\\n\\n\\n\")\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                test_arr = model(X_val_norm.to(device))\n",
    "                test_loss_dict = model.loss_function(test_arr)\n",
    "                test_loss = test_loss_dict['loss'].item()\n",
    "\n",
    "            if test_loss < best_test_loss:\n",
    "\n",
    "                best_test_loss = test_loss\n",
    "                patience_counter = 0\n",
    "\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                \n",
    "                print(f\"Early stopping after {epoch+1} epochs.\")\n",
    "                \n",
    "                print(f'EPOCH : {epoch+1}')\n",
    "                print(f'RECON LOSS : {reconstruction_loss} --- KL LOSS : {kl_divergence_loss} --- LOSS : {loss.item()}')\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                print(\"####################### INPUT IMAGES #######################\")\n",
    "                plot_images((((X_norm.detach().cpu())+1.0)/2.0)[:16])\n",
    "\n",
    "                print(\"####################### RECON IMAGES #######################\")\n",
    "                recons = model.generate(X_norm).detach().cpu()\n",
    "                recons = (recons+1.0)/2.0\n",
    "                plot_images(recons[:16])\n",
    "\n",
    "                print(\"####################### GEN IMAGES #######################\")\n",
    "                sampled = model.sample(num_samples=64, current_device=device)\n",
    "                plot_images((((sampled.detach().cpu())+1.0)/2.0)[:16])\n",
    "\n",
    "                \n",
    "                \n",
    "                print(f\"Early stopping after {epoch+1} epochs.\")\n",
    "                print(\"\\n\\n\\n\")\n",
    "                \n",
    "                \n",
    "                break\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e54793",
   "metadata": {},
   "source": [
    "______________\n",
    "# (c)\n",
    "## Choose the best parameters for VAE (e.g. number of training iterations, learning rate, weightage for KL term, etc.) using the validation set V1 . You can use early stopping criteria and learning rate scheduling based on your choice.\n",
    "\n",
    "# (d)\n",
    "## Clearly describe in your python notebook about the training heuristics used in your code and explain why they were useful.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d5d75",
   "metadata": {},
   "source": [
    "## SOL : (c) & SOL : (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be127f",
   "metadata": {},
   "source": [
    "1. A following code of the form below \n",
    "```\n",
    "    for LR in [1e-4, 2e-4, 3e-4, 5e-4, .001, .002, .003, .005]:\n",
    "        for KLD_WEIGHT in [.0001, .0002, .0005, .00075, .001, .002, .005]:\n",
    "            model.train()\n",
    "```\n",
    "was used to search for the best Learning Rate and weightage for KL term . It is not shown here as the images requires a lot of space.<br><br>\n",
    "\n",
    "2. Batch size were also experimented with  <code> batch_size=64</code> was found to be optimal <br><br>\n",
    "\n",
    "3. Learning Rate Scheduling using <code>StepLR</code> was also done to fine tune the model. Please check the train code above to verify. <br><br>\n",
    "\n",
    "4. Encoder of channels 3 --> 32 --> 64 --> 128 was used. An additional 32 channel layer was added to give more depth to network. This improved the generated quality of the images. <br><br>\n",
    "\n",
    "5. Decoder of the same opposite order were used. The decoder used LeakyRelU as it gave better results, than ReLU. Also at the end a tanh() layer was added to let the image pixel value be in the domain [-1,1] <br><br>\n",
    "\n",
    "6. Early Stopping was implemented usin the same loss function described for train data. The patience parameter was also tuned in order to get descent quality images.<br><br> \n",
    "\n",
    "7. BatchNorm was useful as before that the images were distorted, It is so because the input is in range [-1,1]. There for at every layer the activations should be normalised<br><br>\n",
    "8. The training was done on the augmented dataset that had rotated(+5,-5) and flipped images. This helped increase the dataset size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbe62c",
   "metadata": {},
   "source": [
    "______________\n",
    "# (e)\n",
    "## Clearly describe the objective used for VAE and your training and inference procedures.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c51ff4",
   "metadata": {},
   "source": [
    "## SOL : (e) : Objective"
   ]
  },
  {
   "attachments": {
    "Screenshot%20from%202023-10-31%2018-22-09.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b05398f2",
   "metadata": {},
   "source": [
    "The objective of VAE is made up of two parts : <br>\n",
    "![Screenshot%20from%202023-10-31%2018-22-09.png](attachment:Screenshot%20from%202023-10-31%2018-22-09.png)\n",
    "<br>\n",
    "Now Note that :\n",
    "$$KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "So the KL divergence between q_phi and p_theta has a closed form as they both are normal <br><br>\n",
    "The loss is therefore\n",
    "```\n",
    "recons_loss =F.mse_loss(recons, input)\n",
    "kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "```\n",
    "In order to tune the model a hyperparameter called <code>KLD_WEIGHT</code> is used. So the overall loss used is<br>\n",
    "```\n",
    "total_loss = recon_loss + KLD_WEIGHT X kld_loss\n",
    "```\n",
    "Please verify the claims in the <code> class VAE() </code> above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9aca5",
   "metadata": {},
   "source": [
    "## SOL : (e) : Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f4a9",
   "metadata": {},
   "source": [
    "The inference procedure for VAE is pretty straightforward. Once the VAE is trained, pure Gaussian random noise is passed through the VAE Decoder, and the Generated images are found. This is encapsulated in the function\n",
    "```\n",
    "def sample(self, num_samples, current_device):\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "```\n",
    "which is a part of the VAE Class above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6cceb",
   "metadata": {},
   "source": [
    "______________\n",
    "# (f)\n",
    "## Prepare a plot of the training objective value against the iterations.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9b8d1",
   "metadata": {},
   "source": [
    "## SOL : (f) : Plot is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.grid()\n",
    "plt.title(\"Train Loss vs Epochs\", fontsize=20)\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.xlabel(\"Epochs\", fontsize=12)\n",
    "plt.ylabel(\"Train Loss\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777fe49",
   "metadata": {},
   "source": [
    "______________\n",
    "# (g)\n",
    "## After training, display 10 images (in a 10 × 1 grid) generated using the VAE. For each of the 10 images, display 5 original images from the training data D1 closer to the generated image in a 10 × 5 grid. Comment on the quality of the generated images. Explain the similarity metric you used in the code and justify your choice.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ea3a3",
   "metadata": {},
   "source": [
    "## SOL : (g) : Note the 10x1 grid is drawn along side the closest generated image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3925b8",
   "metadata": {},
   "source": [
    "Images can be visualised as nothing but vectors in a high dimensional space. Therefore it was mose natural to use the MSE distance of images as a similiarity metric. Two images are compared pixewise by substracting their distance squared and the summation is performed over all the pixels of the image. This is a simple yet effective technique as can be seen in the top5 grid below for each generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = model.sample(num_samples=96, current_device=device)\n",
    "X_10 = sampled[0:10]\n",
    "X_10 = X_10.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(12,8))  # Adjust the figsize as needed\n",
    "for i in range(10):\n",
    "    image = ((X_10[i]+1.0)/2.0).permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0049202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILTER TOP 5 ########\n",
    "top5 = []\n",
    "for img in X_10:\n",
    "    mse_score_liss = []\n",
    "    for d_img in D1:\n",
    "        mse_score = F.mse_loss(img, d_img)\n",
    "        mse_score_liss.append(mse_score.item())\n",
    "\n",
    "    top_5_indices = sorted(range(len(mse_score_liss)), key=lambda i: mse_score_liss[i], reverse=True)[:5]\n",
    "    top5.append(top_5_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a311314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATE ALL IMAGES ########\n",
    "all_images = []\n",
    "for idx in range(len(X_10)):\n",
    "    all_images.append(X_10[idx])\n",
    "    for d_img in D1[top5[idx]]:\n",
    "        all_images.append(d_img)\n",
    "        \n",
    "all_images = torch.stack(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bf9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample PyTorch tensor with 60 images shaped 60x3x64x64\n",
    "images = all_images\n",
    "\n",
    "# Create a figure and axis for plotting\n",
    "fig, axes = plt.subplots(nrows=10, ncols=6, figsize=(5, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(6):\n",
    "        ax = axes[i, j]\n",
    "        idx = i * 6 + j\n",
    "        image = images[idx].permute(1, 2, 0).numpy()\n",
    "\n",
    "        if j == 0:\n",
    "            # For the first image in each row, add a thick black outline\n",
    "            ax.imshow((image+1.0)/2.0)\n",
    "            rect = plt.Rectangle((0, 0), 64, 64, linewidth=8, edgecolor='black', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        else:\n",
    "            ax.imshow(image)\n",
    "        if j==0:\n",
    "            ax.set_title(f'Generated', fontsize=10)\n",
    "        else:\n",
    "            ax.set_title(f'Closest{j%6}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38e996",
   "metadata": {},
   "source": [
    "______________\n",
    "# (f)\n",
    "## IMPORTANT: Make sure that your VAE is useful for generating real-looking images. Answers having no meaningful image generation will not be considered for evaluation.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34268b62",
   "metadata": {},
   "source": [
    "Below are the generated images for the trained VAE, please note that these were obtained after a lot of trial and error. Although not very sharphly but some of them do resemble BOAT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_images((((sampled.detach().cpu())+1.0)/2.0)[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995cca4f",
   "metadata": {},
   "source": [
    "_____\n",
    "# ###############THANK YOU###############\n",
    "_____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearn] *",
   "language": "python",
   "name": "conda-env-deeplearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
